"""
Workflow purpose
----------------
Evaluate the performance of different metrics using agglomerative clustering and
different clustering algorithms using the best metric.

Outline
-------
1. Use our benchmark dataset to compute robust independent components with any
available metric in `sklearn` using AgglomerativeClustering:
    - euclidean
    - l1
    - l2
    - manhattan
    - cosine
    - precomputed (abs_pearson)
2. Use benchmark dataset to compute robust independent components with available default clustering algorithms (if distance not precomputed, we always compress dimensions with PCA):
    - Icasso (abs_pearson + AgglomerativeClustering)
    - AffinityPropagation
    - AgglomerativeClustering
    - Birch
    - DBSCAN
    - FeatureAgglomeration
    - KMeans
    - MiniBatchKMeans
    - Meanshift
    - OPTICS
    - SpectralClustering
    - KMedoids
    - CommonNNClustering
3. make figures
"""

import os

##### VARIABLES #####
ROOT = os.path.dirname(os.path.dirname(os.getcwd()))
DATA_DIR = os.path.join(ROOT,'data')
RAW_DIR = os.path.join(DATA_DIR,'raw')
PREP_DIR = os.path.join(DATA_DIR,'prep')
RESULTS_DIR = os.path.join(ROOT,'results','benchmark_clustering')

# variables
CANCER_TYPES = [
    'BRCA',
    'KIRC',
    'LUAD',
    'LUSC',
    'OV',
    'HNSC',
    'GBM',
    'UCEC',
    'THCA', # (Did not converge in AffinityPropagation)
    'PRAD',
    'COAD',
    'LGG',
    'STAD',
    'SKCM',
    'LIHC',
    'BLCA',
    'KIRP',
    'CESC',
    'SARC',
    'ESCA',
    'LAML' 
]


TISSUE_TYPES = [
    'blood',
    'brain', 
    'skin',
    'esophagus',
    'blood_vessel',
    'adipose_tissue', 
    'heart',
    'muscle',
    'lung',
    'colon', 
    'thyroid',
    'nerve',
    'breast',
    'testis',
    'stomach', 
    'pancreas', 
    'pituitary',
    'adrenal_gland', 
    'prostate', 
    'spleen',
    'liver'
]


SAVE_PARAMS = {"sep": "\t", "index": False, "compression": "gzip"}

PROPERTIES_OI = {
    'metrics' : ['euclidean','l1','l2','manhattan','cosine','precomputed'],
    'methods' : [
        'AgglomerativeClustering','AffinityPropagation',
        'DBSCAN','OPTICS','KMedoids','CommonNNClustering'
    ] 
}

ITERATIONS = 100
N_COMPONENTS = 100

DATASETS = ['Sastry2019'] + CANCER_TYPES + TISSUE_TYPES

#### RULES ####
rule all:
    input:
        # evaluate clustering algorithms
        expand(os.path.join(RESULTS_DIR,'files','{dataset}','methods','{property_oi}','performance_evaluation.tsv.gz'), property_oi=PROPERTIES_OI['methods'], dataset=DATASETS),
        expand(os.path.join(RESULTS_DIR,'files','{dataset}','methods','performance_evaluation_merged.tsv.gz'), dataset=DATASETS),
        os.path.join(RESULTS_DIR,'files','clustering_performance_evaluation_merged.tsv.gz'),
        
        # perform PCA
        expand(os.path.join(RESULTS_DIR,'files','{dataset}','pca','components.tsv.gz'), dataset=DATASETS),
        expand(os.path.join(RESULTS_DIR,'files','{dataset}','pca','explained_variance.tsv.gz'), dataset=DATASETS),
        
        # make figures 
        #expand(os.path.join(RESULTS_DIR,'figures','{dataset}','{property_type}'), dataset=["Sastry2019"], property_type=['methods'])
    
    
rule evaluate_clustering_algorithms:
    input:
        S = os.path.join(PREP_DIR,'ica_runs','{dataset}','S.pickle.gz'),
        A = os.path.join(PREP_DIR,'ica_runs','{dataset}','A.pickle.gz')
    output:
        performance = os.path.join(RESULTS_DIR,'files','{dataset}','methods','{property_oi}','performance_evaluation.tsv.gz'),
        clustering = os.path.join(RESULTS_DIR,'files','{dataset}','methods','{property_oi}','clustering_info.tsv.gz')
    params:
        property_type = 'methods',
        properties_oi = '{property_oi}',
        iterations = ITERATIONS
    threads: 1
    resources:
        runtime = 3600*6, # 6h
        memory = 16 # GB
    shell:
        """
        nice python scripts/evaluate_clustering.py \
                    --S_all_file={input.S} \
                    --A_all_file={input.A} \
                    --property_type={params.property_type} \
                    --properties_oi={params.properties_oi} \
                    --output_file={output.performance} \
                    --iterations={params.iterations} \
        """
        

rule merge_clustering_algorithms:
    input:
        performance = [
            os.path.join(RESULTS_DIR,'files','{dataset}','methods',
                         '{property_oi}','performance_evaluation.tsv.gz'
                        ).format(property_oi=property_oi, dataset="{dataset}") 
         for property_oi in PROPERTIES_OI['methods']],
        clustering = [
            os.path.join(RESULTS_DIR,'files','{dataset}','methods',
                         '{property_oi}','clustering_info.tsv.gz'
                        ).format(property_oi=property_oi, dataset="{dataset}") 
         for property_oi in PROPERTIES_OI['methods']]
    output:
        performance = os.path.join(RESULTS_DIR,'files','{dataset}','methods','performance_evaluation_merged.tsv.gz'),
        clustering = os.path.join(RESULTS_DIR,'files','{dataset}','methods','clustering_info_merged.tsv.gz')
    threads: 1
    resources:
        runtime = int(3600*0.5), # 0.5h
        memory = 6 # GB
    run:
        import os
        import pandas as pd
        for key, filenames in input.items():
            df = pd.concat([
                pd.read_table(
                    filename
                 ).assign(dataset = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(filename)))))
                 for filename in filenames])
            df.to_csv(output[key], **SAVE_PARAMS)
        print('Done!')
        
        
        
rule merge_clustering_across_datasets:
    input:
        performance = [os.path.join(RESULTS_DIR,'files','{dataset}','methods','performance_evaluation_merged.tsv.gz').format(dataset=dataset) for dataset in DATASETS],
        clustering = [os.path.join(RESULTS_DIR,'files','{dataset}','methods','clustering_info_merged.tsv.gz').format(dataset=dataset) for dataset in DATASETS]
    output:
        performance = os.path.join(RESULTS_DIR,'files','clustering_performance_evaluation_merged.tsv.gz'),
        clustering = os.path.join(RESULTS_DIR,'files','clustering_info_merged.tsv.gz')
    run:
        import pandas as pd
        for key, filenames in input.items():
            df = pd.concat([ pd.read_table(filename) for filename in filenames ])
            df.to_csv(output[key], **SAVE_PARAMS)
        print('Done!')
        
        
rule pca_ica_runs:
    input:
        S = os.path.join(PREP_DIR,'ica_runs','{dataset}','S.pickle.gz')
    output:
        components = os.path.join(RESULTS_DIR,'files','{dataset}','pca','components.tsv.gz'),
        explained_variance = os.path.join(RESULTS_DIR,'files','{dataset}','pca','explained_variance.tsv.gz')
    params:
        n_components = 10
    run:
        import pandas as pd
        from sklearn.decomposition import PCA
        
        # unpack parameters
        n_components = params.n_components
        
        # run PCA
        X = pd.read_pickle(input.S)
        pca = PCA(n_components = n_components)
        pca.fit(X)
        
        # prepare outputs
        components = pd.DataFrame(pca.components_,
                                  index = ["PC%s" % (comp+1) for comp in range(n_components)],
                                  columns = X.columns).T
        explained_variance = pd.DataFrame({
            "component": ["PC%s" % (comp+1) for comp in range(n_components)],
            "explained_variance_ratio": pca.explained_variance_ratio_
        })
        
        # save
        components.to_csv(output.components, sep="\t", index=False, compression="gzip")
        explained_variance.to_csv(output.explained_variance, sep="\t", index=False, compression="gzip")
        
        
    
rule figures_benchmark_clustering:
    input:
        performance = os.path.join(RESULTS_DIR,'files','{dataset}','{property_type}','performance_evaluation_merged.tsv.gz'),
        clustering = os.path.join(RESULTS_DIR,'files','{dataset}','{property_type}','clustering_info_merged.tsv.gz'),
    output:
        directory(os.path.join(RESULTS_DIR,'figures','{dataset}','{property_type}'))
    shell:
        """
        Rscript scripts/figures_benchmark_clustering.R \
                    --performance_file={input.performance} \
                    --clustering_file={input.clustering} \
                    --figs_dir={output}
        """
