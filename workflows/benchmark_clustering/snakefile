"""
Workflow purpose
----------------
Evaluate the performance of different clustering algorithms.
"""

import os

##### VARIABLES #####
ROOT = os.path.dirname(os.path.dirname(os.getcwd()))
DATA_DIR = os.path.join(ROOT,'data')
RAW_DIR = os.path.join(DATA_DIR,'raw')
PREP_DIR = os.path.join(DATA_DIR,'prep')
RESULTS_DIR = os.path.join(ROOT,'results','benchmark_clustering')

# variables
CANCER_TYPES = [
    'BRCA',
    'KIRC',
    'LUAD',
    'LUSC',
    'OV',
    'HNSC',
    'GBM',
    'UCEC',
    'THCA',
    'PRAD',
    'COAD',
    'LGG',
    'STAD',
    'SKCM',
    'LIHC',
    'BLCA',
    'KIRP',
    'CESC',
    'SARC',
    'ESCA',
    'LAML' 
]


TISSUE_TYPES = [
    'blood',
    'brain', 
    'skin',
    'esophagus',
    'blood_vessel',
    'adipose_tissue', 
    'heart',
    'muscle',
    'lung',
    'colon', 
    'thyroid',
    'nerve',
    'breast',
    'testis',
    'stomach', 
    'pancreas', 
    'pituitary',
    'adrenal_gland', 
    'prostate', 
    'spleen',
    'liver'
]


SAVE_PARAMS = {"sep": "\t", "index": False, "compression": "gzip"}

PROPERTIES_OI = {
    'metrics' : ['euclidean','l1','l2','manhattan','cosine','precomputed'],
    'methods' : [
        'AgglomerativeClustering','AffinityPropagation',
        'DBSCAN','OPTICS','KMedoids','CommonNNClustering'
    ] 
}

ITERATIONS = 100
N_COMPONENTS = 100

DATASETS = ['Sastry2019'] + CANCER_TYPES + TISSUE_TYPES

#### RULES ####
rule all:
    input:
        # evaluate clustering algorithms
        expand(os.path.join(RESULTS_DIR,'files','{dataset}','methods','{property_oi}','performance_evaluation.tsv.gz'), property_oi=PROPERTIES_OI['methods'], dataset=DATASETS),
        expand(os.path.join(RESULTS_DIR,'files','{dataset}','methods','performance_evaluation_merged.tsv.gz'), dataset=DATASETS),
        os.path.join(RESULTS_DIR,'files','clustering_performance_evaluation_merged.tsv.gz'),
        
        # perform PCA
        expand(os.path.join(RESULTS_DIR,'files','{dataset}','pca','components.tsv.gz'), dataset=DATASETS),
        expand(os.path.join(RESULTS_DIR,'files','{dataset}','pca','explained_variance.tsv.gz'), dataset=DATASETS),
        
        # make figures 
        os.path.join(RESULTS_DIR,'figures','Sastry2019'),
        os.path.join(RESULTS_DIR,'figures','all')
    
    
rule evaluate_clustering_algorithms:
    input:
        S = os.path.join(PREP_DIR,'ica_runs','{dataset}','S.pickle.gz'),
        A = os.path.join(PREP_DIR,'ica_runs','{dataset}','A.pickle.gz')
    output:
        performance = os.path.join(RESULTS_DIR,'files','{dataset}','methods','{property_oi}','performance_evaluation.tsv.gz'),
        clustering = os.path.join(RESULTS_DIR,'files','{dataset}','methods','{property_oi}','clustering_info.tsv.gz')
    params:
        property_type = 'methods',
        properties_oi = '{property_oi}',
        iterations = ITERATIONS
    threads: 1
    resources:
        runtime = 3600*6, # 6h
        memory = 16 # GB
    shell:
        """
        nice python scripts/evaluate_clustering.py \
                    --S_all_file={input.S} \
                    --A_all_file={input.A} \
                    --property_type={params.property_type} \
                    --properties_oi={params.properties_oi} \
                    --output_file={output.performance} \
                    --iterations={params.iterations} \
        """
        

rule merge_clustering_algorithms:
    input:
        performance = [
            os.path.join(RESULTS_DIR,'files','{dataset}','methods',
                         '{property_oi}','performance_evaluation.tsv.gz'
                        ).format(property_oi=property_oi, dataset="{dataset}") 
         for property_oi in PROPERTIES_OI['methods']],
        clustering = [
            os.path.join(RESULTS_DIR,'files','{dataset}','methods',
                         '{property_oi}','clustering_info.tsv.gz'
                        ).format(property_oi=property_oi, dataset="{dataset}") 
         for property_oi in PROPERTIES_OI['methods']]
    output:
        performance = os.path.join(RESULTS_DIR,'files','{dataset}','methods','performance_evaluation_merged.tsv.gz'),
        clustering = os.path.join(RESULTS_DIR,'files','{dataset}','methods','clustering_info_merged.tsv.gz')
    threads: 1
    resources:
        runtime = int(3600*0.5), # 0.5h
        memory = 6 # GB
    run:
        import os
        import pandas as pd
        for key, filenames in input.items():
            df = pd.concat([
                pd.read_table(
                    filename
                 ).assign(dataset = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(filename)))))
                 for filename in filenames])
            df.to_csv(output[key], **SAVE_PARAMS)
        print('Done!')
        
        
        
rule merge_clustering_across_datasets:
    input:
        performance = [os.path.join(RESULTS_DIR,'files','{dataset}','methods','performance_evaluation_merged.tsv.gz').format(dataset=dataset) for dataset in DATASETS],
        clustering = [os.path.join(RESULTS_DIR,'files','{dataset}','methods','clustering_info_merged.tsv.gz').format(dataset=dataset) for dataset in DATASETS]
    output:
        performance = os.path.join(RESULTS_DIR,'files','clustering_performance_evaluation_merged.tsv.gz'),
        clustering = os.path.join(RESULTS_DIR,'files','clustering_info_merged.tsv.gz')
    run:
        import pandas as pd
        for key, filenames in input.items():
            df = pd.concat([ pd.read_table(filename) for filename in filenames ])
            df.to_csv(output[key], **SAVE_PARAMS)
        print('Done!')
        
        
rule pca_ica_runs:
    input:
        S = os.path.join(PREP_DIR,'ica_runs','{dataset}','S.pickle.gz')
    output:
        components = os.path.join(RESULTS_DIR,'files','{dataset}','pca','components.tsv.gz'),
        explained_variance = os.path.join(RESULTS_DIR,'files','{dataset}','pca','explained_variance.tsv.gz')
    params:
        n_components = 10
    run:
        import pandas as pd
        from sklearn.decomposition import PCA
        
        # unpack parameters
        n_components = params.n_components
        
        # run PCA
        X = pd.read_pickle(input.S)
        pca = PCA(n_components = n_components)
        pca.fit(X)
        
        # prepare outputs
        components = pd.DataFrame(pca.components_,
                                  index = ["PC%s" % (comp+1) for comp in range(n_components)],
                                  columns = X.columns).T
        explained_variance = pd.DataFrame({
            "component": ["PC%s" % (comp+1) for comp in range(n_components)],
            "explained_variance_ratio": pca.explained_variance_ratio_
        })
        
        # save
        components.to_csv(output.components, sep="\t", index=False, compression="gzip")
        explained_variance.to_csv(output.explained_variance, sep="\t", index=False, compression="gzip")
        
        
    
rule figures_benchmark_clustering_all:
    input:
        performance = os.path.join(RESULTS_DIR,'files','clustering_performance_evaluation_merged.tsv.gz'),
        clustering = os.path.join(RESULTS_DIR,'files','clustering_info_merged.tsv.gz'),
        dataset_info = os.path.join(ROOT,'results','preprocess_data','files','dataset_info.tsv')
    output:
        directory(os.path.join(RESULTS_DIR,'figures','all'))
    shell:
        """
        nice Rscript scripts/figures_benchmark_clustering_all.R \
                    --performance_file={input.performance} \
                    --clustering_file={input.clustering} \
                    --dataset_info_file={input.dataset_info} \
                    --figs_dir={output}
        """
        
        
rule figures_benchmark_clustering_Sastry2019:
    input:
        performance = os.path.join(RESULTS_DIR,'files','Sastry2019','methods','performance_evaluation_merged.tsv.gz'),
        clustering = os.path.join(RESULTS_DIR,'files','Sastry2019','methods','clustering_info_merged.tsv.gz'),
        pca_components = os.path.join(RESULTS_DIR,'files','Sastry2019','pca','components.tsv.gz'),
        components_robustica = os.path.join(RESULTS_DIR,'files','Sastry2019','methods','DBSCAN','DBSCAN-S.tsv.gz'),
        components_Sastry2019 = os.path.join(PREP_DIR,'original_pipeline_Sastry2019','original','S.csv')
    output:
        directory(os.path.join(RESULTS_DIR,'figures','Sastry2019'))
    shell:
        """
        nice Rscript scripts/figures_benchmark_clustering_Sastry2019.R \
                    --performance_file={input.performance} \
                    --clustering_file={input.clustering} \
                    --pca_components_file={input.pca_components} \
                    --components_robustica_file={input.components_robustica} \
                    --components_Sastry2019_file={input.components_Sastry2019} \
                    --figs_dir={output}
        """

